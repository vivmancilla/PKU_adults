###DADA2 16S Amplicon Processing####
#This script was modified from Allie Mann's DADA2 and vizualization scripts
#When running this script it's best to run it interactively in the R environment to be sure each step works as expected
#there are also points within the protocol where you need to examine some output files to make decisions about quality
#filtering, trimming, etc. Once you've run through the script and are sure everything is up to snuff you can re-run
#from the terminal using: Rscript dada2.for_16s_dat.R whenever you need to (e.g., you noticed a mistake and you need to reprocess)
#required packages: QIIME2, mafft, VSEARCH, CutAdapt, SeqTK, fasttree

#may need to start R in the following way if you encounter vector memory full error -- can also add to bash profile: env R_MAX_VSIZE=700Gb Rscript dada2_processing.3.6.1.R

####CHANGE THESE####
PATH="/home/lymelab/Desktop/PKU2018_adults/" #CHANGE ME to your working directory
RAW="/home/lymelab/Desktop/PKU2018_adults" #CHANGE ME to where your raw fastq files are
FWPRI="GTGYCAGCMGCCGCGGTAA" #CHANGE ME to your forward primer
RVPRI="GGACTACNVGGGTWTCTAAT" #CHANGE ME to your reverse primer
CUTAD="/usr/bin/cutadapt" #CHANGE ME to your cutadapt install (if you don't know where this is try the following command in your terminal: which cutadapt)
FPAT="_R1_001.fastq.gz" #CHANGE ME to match the pattern in your forward reads
RVPAT="_R2_001.fastq.gz" #CHANGE ME to match the pattern in your reverse reads
VSEARCH="/usr/bin/vsearch"
BACTAX="/home/lymelab/Desktop/referenceDB/EzBioCloud_16S_database_for_QIIME/ezbiocloud_id_taxonomy.qza" #change this to your bacterial reference db taxonomy file
BACREF="/home/lymelab/Desktop/referenceDB/EzBioCloud_16S_database_for_QIIME/ezbiocloud_qiime_full.qza" #change this to your bacterial reference db fasta file
#####################

####Required Libraries####
library(dada2)
library(tidyverse)
library(reshape2)
library(stringr)
library(data.table)
library(broom)
library(qualpalr)
library(viridis)
library(ShortRead)
library(Biostrings)
library(seqinr)
library(phyloseq)

####Environment Setup####
theme_set(theme_bw())
setwd(PATH)

####File Path Setup####
#this is so dada2 can quickly iterate through all the R1 and R2 files in your read set
path <- RAW
list.files(path)
fnFs <- sort(list.files(path, pattern=FPAT, full.names = TRUE))
fnRs <- sort(list.files(path, pattern=RVPAT, full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1) #CHANGE the delimiter in quotes and the number at the end of this command to decide how to split up the file name, and which element to extract for a unique sample name

####fastq Quality Plots####
# be sure to take a look at these, they should inform your read trimming decisions downstream
pdf(paste(PATH,"forward_quality_plot.pdf", sep=""))
plotQualityProfile(fnFs[1:10]) #this plots the quality profiles for each sample, if you have a lot of samples, it's best to look at just a few of them, the plots take a minute or two to generate even only showing 10-20 samples.
dev.off()
pdf(paste(PATH,"reverse_quality_plot.pdf",sep=""))
plotQualityProfile(fnRs[1:10])
dev.off()

####Primer Removal####
####identify primers####
FWD <- FWPRI
REV <- RVPRI
allOrients <- function(primer) {
  # Create all orientations of the input sequence
  require(Biostrings)
  dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna),
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients

fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE, compress = TRUE)

primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[2]]), #add the index of the sample you'd like to use for this test (your first sample may be a blank/control and not have many sequences in it, be mindful of this)
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[2]]),
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[2]]),
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[2]]))

#### primer removal ####
cutadapt <- CUTAD
system2(cutadapt, args = "--version")

path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC)
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC)

#Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i]), "1>cutadapt.log") # input files #see if log works if not remove
}

#sanity check, should report zero for all orientations and read sets
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]),
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]),
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]),
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))

# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "R1", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "R2", full.names = TRUE))

####filter and trim reads####
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))

####trim & filter####
#filter and trim command. dada2 can canonically handle lots of errors, I am typically permissive in the maxEE parameter set here, in order to retain the maximum number of reads possible. error correction steps built into the dada2 pipeline have no trouble handling data with this many expected errors.
#check out the DADA2 tutorial to see what each of the options in the command below do
out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, trimRight=50, minLen = c(150,120),
                     maxN=c(0,0), maxEE=c(2,2), truncQ=c(2,2), rm.phix=TRUE, matchIDs=TRUE,
                     compress=TRUE, multithread=TRUE)
retained <- as.data.frame(out)
retained$percentage_retained <- retained$reads.out/retained$reads.in*100
#check this table, you should retain most of your reads. If you lose a lot you might need to change the filterAndTrim quality thresholds
View(retained)

#the next three sections (learn error rates, dereplication, sample inference) are the core of dada2's sequence processing pipeline.
####learn error rates####
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)

pdf(paste(PATH, "error_plot.pdf", sep=""))
plotErrors(errF, nominalQ=TRUE) #assess this graph. it shows the error rates observed in your dataset. strange or unexpected shapes in the plot should be considered before moving on.
dev.off()

####dereplication####
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)

# Name the derep-class objects by the sample names
#this is just to ensure that all your R objects have the same sample names in them
names(derepFs) <- sample.names
names(derepRs) <- sample.names

####sample inference####
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)

dadaFs[[1]]
dadaRs[[1]]

####OPTIONAL: remove low-sequence samples before merging####
#a "subscript out of bounds" error at the next step (merging) may indicate that you aren't merging any reads in one or more samples.
#samples_to_keep <- as.numeric(out[,"reads.out"]) > 500 #example of simple method used above after the filter and trim step. if you already did this but still got an error when merging, try the steps below
getN <- function(x) sum(getUniques(x)) #keeping track of read retention, number of unique sequences after ASV inference
track <- cbind(sapply(derepFs, getN), sapply(derepRs, getN), sapply(dadaFs, getN), sapply(dadaRs, getN))
samples_to_keep <- track[,4] > 500 #your threshold. try different ones to get the lowest one that will work. #this method accounts for dereplication/ASVs left after inference
samples_to_remove <- names(samples_to_keep)[which(samples_to_keep == FALSE)] #record names of samples you have the option of removing

####merge paired reads####
mergers <- mergePairs(dadaFs[samples_to_keep], derepFs[samples_to_keep], dadaRs[samples_to_keep], derepRs[samples_to_keep], verbose=TRUE)

# Inspect the merger data.frame from the first sample
head(mergers[[1]])

####construct sequence table####
seqtab <- makeSequenceTable(mergers)
dim(seqtab)

####View Sequence Length Distribution Post-Merging####
#most useful with merged data. this plot will not show you much for forward reads only, which should have a uniform length distribution.
length.histogram <- as.data.frame(table(nchar(getSequences(seqtab)))) #tabulate sequence length distribution
pdf(paste(PATH,"length_hist.pdf",sep=""))
plot(x=length.histogram[,1], y=length.histogram[,2]) #view length distribution plot
dev.off()

####remove low-count singleton ASVs####
#create phyloseq otu_table
otus <- otu_table(t(seqtab), taxa_are_rows = TRUE)

#some metrics from the sequence table
otu_pres_abs <- otus
otu_pres_abs[otu_pres_abs >= 1] <- 1 #creating a presence/absence table
otu_pres_abs_rowsums <- rowSums(otu_pres_abs) #counts of sample per ASV
length(otu_pres_abs_rowsums) #how many ASVs
length(which(otu_pres_abs_rowsums == 1)) #how many ASVs only present in one sample

#what are the counts of each ASV
otu_rowsums <- rowSums(otus) #raw counts per ASV
otu_singleton_rowsums <- as.data.frame(otu_rowsums[which(otu_pres_abs_rowsums == 1)]) #raw read counts in ASVs only presesnt in one sample
hist(otu_singleton_rowsums[,1], breaks=500, xlim = c(0,200), xlab="# Reads in ASV") #histogram plot of above
length(which(otu_singleton_rowsums <= 50)) #how many ASVs are there with N reads or fewer? (N=50 in example)

#IF you want to filter out rare variants (low-read-count singleton ASVs) you can use phyloseq's "transform_sample_counts" to create a relative abundance table, and then filter your ASVs by choosing a threshold of relative abundance: otus_rel_ab = transform_sample_counts(otus, function(x) x/sum(x))
dim(seqtab) # sanity check
dim(otus) # (this should be the same as last command, but the dimensions reversed)
otus_rel_ab <- transform_sample_counts(otus, function(x) x/sum(x)) #create relative abundance table
df <- as.data.frame(unclass(otus_rel_ab)) #convert to plain data frame
df[is.na(df)] <- 0 #if there are samples with no merged reads in them, and they passed the merge step (a possiblity, converting to a relative abundance table produes all NaNs for that sample. these need to be set to zero so we can do the calculations in the next steps.)
otus_rel_ab.rowsums <- rowSums(df) #compute row sums (sum of relative abundances per ASV. for those only present in one sample, this is a value we can use to filter them for relative abundance on a per-sample basis)
a <- which(as.data.frame(otu_pres_abs_rowsums) == 1) #which ASVs are only present in one sample
b <- which(otus_rel_ab.rowsums <= 0.001) #here is where you set your relative abundance threshold #which ASVs pass our filter for relative abundance
length(intersect(a,b)) #how many of our singleton ASVs fail on this filter
rows_to_remove <- intersect(a,b) #A also in B (we remove singleton ASVs that have a lower relative abundance value than our threshold)
otus_filt <- otus[-rows_to_remove,] #filter OTU table we created earlier
dim(otus_filt) #how many ASVs did you retain?
seqtab.nosingletons <- t(as.matrix(unclass(otus_filt))) #convert filtered OTU table back to a sequence table matrix to continue with dada2 pipeline

####remove chimeras####
#here we remove "bimeras" or chimeras with two sources. look at "method" to decide which type of pooling you'd like to use when judging each sequence as chimeric or non-chimeric
seqtab.nosingletons.nochim <- removeBimeraDenovo(seqtab.nosingletons, method="pooled", multithread=FALSE, verbose=TRUE) #this step can take a few minutes to a few hours, depending on the size of your dataset
dim(seqtab.nosingletons.nochim)
sum(seqtab.nosingletons.nochim)/sum(seqtab.nosingletons) #proportion of nonchimeras #it should be relatively high after filtering out your singletons/low-count ASVs, even if you lose a lot of ASVs, the number of reads lost should be quite low

####track read retention through steps####
getN <- function(x) sum(getUniques(x))
track <- cbind(out[samples_to_keep,], sapply(dadaFs[samples_to_keep], getN), sapply(dadaRs[samples_to_keep], getN), sapply(mergers, getN), rowSums(seqtab.nosingletons), rowSums(seqtab.nosingletons.nochim))
# If processing only a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
track <- cbind(track, 100-track[,6]/track[,5]*100, 100-track[,7]/track[,6]*100)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nosingletons", "nochimeras", "percent_singletons", "percent_chimeras")
rownames(track) <- sample.names[samples_to_keep]

####save output from sequnce table construction steps####
write.table(data.frame("row_names"=rownames(track),track),"read_retention.16s.txt", row.names=FALSE, quote=F, sep="\t")
write.table(data.frame("row_names"=rownames(seqtab.nosingletons.nochim),seqtab.nosingletons.nochim),"sequence_table.16s.merged.txt", row.names=FALSE, quote=F, sep="\t")
uniquesToFasta(seqtab.nosingletons.nochim, "rep_set.fa")
system("awk '/^>/{print \">ASV\" ++i; next}{print}' < rep_set.fa > rep_set_fix.fa")

################Load back into R

#if you must save your sequence table and load it back in before doing taxonomy assignments, here is how to reformat the object so that dada2 will accept it again
seqtab.nosingletons.nochim <- fread("sequence_table.16s.merged.txt", sep="\t", header=T, colClasses = c("row_names"="character"), data.table=FALSE)
row.names(seqtab.nosingletons.nochim) <- seqtab.nosingletons.nochim[,1] #set row names
seqtab.nosingletons.nochim <- seqtab.nosingletons.nochim[,-1] #remove column with the row names in it
seqtab.nosingletons.nochim <- as.matrix(seqtab.nosingletons.nochim) #cast the object as a matrix

# now replace the long ASV names (the actual sequences) with human-readable names, and save the new names and sequences as a .fasta file in your project working directory
my_otu_table <- t(as.data.frame(seqtab.nosingletons.nochim)) #transposed (OTUs are rows) data frame. unclassing the otu_table() output avoids type/class errors later on
ASV.seq <- as.character(unclass(row.names(my_otu_table))) #store sequences in character vector
ASV.num <- paste0("ASV", seq(ASV.seq), sep='') #create new names
colnames(seqtab.nosingletons.nochim) <- ASV.num #rename your ASVs in the taxonomy table and sequence table objects

##assign taxonomy using VSEARCH and QIIME2
#first need to convert reference sequences to qiime2 format
#reference database should be in qiime format: qiime tools import --input-path ezbiocloud_qiime_full.fasta --output-path ezbiocloud_qiime_full.qza --type 'FeatureData[Sequence]'
#also taxonomy file: qiime tools import --type 'FeatureData[Taxonomy]' --input-format HeaderlessTSVTaxonomyFormat --input-path ezbiocloud_id_taxonomy.txt --output-path ezbiocloud_id_taxonomy.qza
#pull the region of interest from full length 16S sequences (is better for classification): qiime feature-classifier extract-reads --i-sequences ezbiocloud_qiime_full.qza --p-f-primer GTGYCAGCMGCCGCGGTAA --p-r-primer GGACTACNVGGGTWTCTAAT --p-min-length 200 --p-max-length 300 --o-reads ezbiocloud_qiime_v4region.qza
system("/home/lymelab/miniconda2/envs/qiime2-2019.7/bin/qiime tools import --input-path rep_set_fix.fa --output-path rep_set_fix.qza --type 'FeatureData[Sequence]'")
system("rm -r assigntax")
system(sprintf("/home/lymelab/miniconda2/envs/qiime2-2019.7/bin/qiime feature-classifier classify-consensus-vsearch --i-query rep_set_fix.qza --i-reference-reads %s --i-reference-taxonomy %s --output-dir assigntax", BACREF, BACTAX))
system("unzip assigntax/classification.qza -d assigntax/")

#get file path for taxonomy file
tempfile <-
  subset(dir(path="assigntax"),
         !grepl("classification.qza",
                dir(path="assigntax/")))
tempfile # check output of this if it isn't a weird directory name, change value above to one
newpath <- paste("assigntax/", tempfile, "/data/taxonomy.tsv", sep="") ###might have to change number in brackets

#make 7 level taxonomy file
system(sprintf("awk '{print $2}' %s | sed '1d' > taxonomy_strings.txt", newpath))
system(sprintf("awk '{print $1}' %s | sed '1d' > asv_ids.txt", newpath))
system("python /usr/bin/fix_taxonomy_L7.py taxonomy_strings.txt > fix_string.txt")
system("paste asv_ids.txt fix_string.txt > taxonomy_L7.txt")
system("rm taxonomy_strings.txt fix_string.txt asv_ids.txt")
system("sed -i 's/;/\t/g' taxonomy_L7.txt")

####combine sequence and taxonomy tables into one####
#taxa will be the rows, columns will be samples, followed by each rank of taxonomy assignment, from rank1 (domain-level) to rank7/8 (species-level), followed by accession (if applicable)
#first check if the row names of the taxonomy table match the column headers of the sequence table
taxa <- read.table(newpath, header=T, sep="\t", row.names=1)
length(which(row.names(taxa) %in% colnames(seqtab.nosingletons.nochim)))
dim(taxa)
dim(seqtab.nosingletons.nochim)
#the number of taxa from the last three commands should match

#now ensure that the taxa in the tables are in the same order #this should be true if you haven't reordered one or the other of these matrices inadvertently
order.col <- row.names(taxa)
seqtab.nosingletons.nochim <- seqtab.nosingletons.nochim[,order.col]
row.names(taxa) == colnames(seqtab.nosingletons.nochim) #IMPORTANT: only proceed if this evaluation is true for every element. if it isn't you'll need to re-order your data. I'd suggest sorting both matrices by their rows after transposing the sequence table.

#as long as the ordering of taxa is the same, you can combine like this (note you need to transpose the sequence table so that the taxa are in the rows)
sequence_taxonomy_table <- cbind(t(seqtab.nosingletons.nochim), taxa)
colnames(sequence_taxonomy_table)[colnames(sequence_taxonomy_table)=="V2"] <- "taxonomy"
colnames(sequence_taxonomy_table)[colnames(sequence_taxonomy_table)=="V3"] <- "score"
colnames(sequence_taxonomy_table)[colnames(sequence_taxonomy_table)=="V4"] <- "count"

#now write to file
write.table(data.frame("row_names"=rownames(sequence_taxonomy_table),sequence_taxonomy_table),"sequence_taxonomy_table.16s.merged.txt", row.names=FALSE, quote=F, sep="\t")

#filter out unwanted taxonomic groups
system("grep -v 'Unassigned' sequence_taxonomy_table.16s.merged.txt | awk '{print $1}' | grep 'A' > wanted.ids")
wanted <- read.table("wanted.ids", header=F)
seqtab.filtered <- seqtab.nosingletons.nochim[, which(colnames(seqtab.nosingletons.nochim) %in% wanted$V1)]
write.table(data.frame("row_names"=rownames(seqtab.filtered),seqtab.filtered),"sequence_table.16s.filtered.txt", row.names=FALSE, quote=F, sep="\t")

#get representative seq tree
system("seqtk subseq rep_set_fix.fa wanted.ids > rep_set.filt.fa")
system("mafft --auto rep_set.filt.fa > rep_set.filt.align.fa")
system("fasttree -nt rep_set.filt.align.fa > rep_set.filt.tre")

#############################################
#############Visualization###################
#############################################

###########
#LIBRARIES
###########
library(tidyverse)
library(ggplot2)
library(ggdendro)
library(gridExtra)
library(dendextend)
library(compositions)
library(ape)
library(factoextra)
library(RColorBrewer)
library(philr)
library(phyloseq)
library(UpSetR)
library(reshape2)
library(vegan)
library(phylofactor)
library(ggtree)
library(DESeq2)

##########
#ENV SETUP
##########
PATH="/home/lymelab/Desktop/PKU2018_adults/"
setwd(PATH)
cols <- brewer.pal(6, "Set2")

###########
#LOAD DATA
###########
#load sample data
rawmetadata <- read_delim(file = file.path(PATH, "map.txt"), # file.path() is used for cross-platform compatibility
                          "\t", # the text file is tab delimited
                          escape_double = FALSE, # the imported text file does not 'escape' quotation marks by wrapping them with more quotation marks
                          trim_ws = TRUE) # remove leading and trailing spaces from character string entries
#if you come across subscript out of bounds error, see if transposing and changing taxa_are_rows command will fix
seqtab.filtered <- read.table("sequence_table.16s.filtered.txt", header=T, row.names=1)
taxa <- read.table("taxonomy_L7.txt", header=F, sep="\t", row.names=1)
notinmeta <- setdiff(colnames(seqtab.filtered), rawmetadata$SampleID) #record samples absent in either metadata or OTU table
notinraw <- setdiff(rawmetadata$SampleID, row.names(seqtab.filtered))
tree <- read.tree("rep_set.filt.root.tre") #load representative tree tree must be rooted or unifrac will cry
rownames(rawmetadata) <- rawmetadata$SampleID

ps.dat <- phyloseq(otu_table(seqtab.filtered, taxa_are_rows=FALSE),
                   sample_data(rawmetadata),
                   tax_table(as.matrix(taxa)), tree)

philr.dat <- transform_sample_counts(ps.dat, function(x) x+1) #add pseudocount of one to OTUs to avoid log-ratios involving zeros
is.rooted(phy_tree(philr.dat)) #check that tree is rooted
is.binary.tree(phy_tree(philr.dat)) #check that multichotomies are resolved in tree
phy_tree(philr.dat) <- makeNodeLabel(phy_tree(philr.dat), method="number", prefix="n")
otu.table <- otu_table(philr.dat)
tree <- phy_tree(philr.dat)
metadata <- sample_data(philr.dat)
tax <- tax_table(philr.dat)
philr.t <- philr(otu.table, tree, part.weights="enorm.x.gm.counts", ilr.weights="blw.sqrt")

# Heirarchical cluster dendrogram
hc <- hclust(dist(philr.t), method="complete")
df2 <- data.frame(cluster=cutree(hc,5), states=factor(hc$labels, levels=hc$labels[hc$order])) # get cluster assocaited with each sample
write.table(df2, "philr_cluster.txt", quote=F, sep="\t", col.names=NA)

hcd <- as.dendrogram(hc)
dend_data <- dendro_data(hcd, type="rectangle")
p1 <- ggplot(dend_data$segments) + geom_segment(aes(x=x,y=y, xend=xend, yend=yend)) + theme_classic() + geom_text(data = dend_data$labels, aes(x, y, label = label, hjust = 1, angle = 90)) + xlab("") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
merge <- merge(df2, rawmetadata, by.x=c("states"), by.y=c("SampleID"))
p2 <- ggplot(merge, aes(states, y=1, fill=factor(merge$cluster))) + geom_tile() + scale_fill_manual(values=cols) + scale_y_continuous(expand=c(0,0)) + theme(axis.title=element_blank(), axis.ticks=element_blank(), axis.text=element_blank(), legend.position="none")
gp1<-ggplotGrob(p1)
gp2<-ggplotGrob(p2)
maxWidth <- grid::unit.pmax(gp1$widths[2:5], gp2$widths[2:5])
gp1$widths[2:5] <- as.list(maxWidth)
gp2$widths[2:5] <- as.list(maxWidth)
system("mkdir figs")
pdf("figs/philr_dendrogram.pdf")
grid.arrange(gp1, gp2, ncol=1,heights=c(4/5,1/5,1/5))
dev.off()

# PCA
philr.dist <- dist(philr.t, method="euclidean")
pca <- prcomp(as.matrix(philr.dist))
pdf("figs/philr_screeplot_.pdf")
screeplot(pca)
dev.off()
pdf("figs/philr_pca.pdf")
fviz_pca_ind(pca, habillage=merge$Group, geom="point", pointshape=19, point.size=1, mean.point=T) + labs(title="PhILR Distance") + scale_color_brewer(palette="Dark2") + theme_minimal()
dev.off()

ps.dat <- phyloseq(otu_table(seqtab.filtered, taxa_are_rows=FALSE),
                   sample_data(rawmetadata),
                   tax_table(as.matrix(taxa[1])), tree)

# PERMANOVA
metadata <- as(sample_data(ps.dat), "data.frame")
adonis(philr.dist ~ Group, data=metadata)

##########################
#WEIGHTED UNIFRAC DISTANCE
##########################
ps.dada2_join.rare <- rarefy_even_depth(ps.dat, sample.size=15000, rngseed=456)
wuni <- UniFrac(ps.dada2_join.rare, weighted=T)
hc <- hclust(wuni, method="complete")
df2 <- data.frame(cluster=cutree(hc,10), states=factor(hc$labels, levels=hc$labels[hc$order])) # get cluster assocaited with each sample
hcd <- as.dendrogram(hc)
dend_data <- dendro_data(hcd, type="rectangle")
p1 <- ggplot(dend_data$segments) + geom_segment(aes(x=x,y=y, xend=xend, yend=yend)) + theme_classic() + geom_text(data = dend_data$labels, aes(x, y, label = label, hjust = 1, angle = 90)) + xlab("") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
merge <- merge(df2, rawmetadata, by.x=c("states"), by.y=c("SampleID"))
p2 <- ggplot(merge, aes(states, y=1, fill=factor(merge$cluster))) + geom_tile() + scale_y_continuous(expand=c(0,0)) + theme(axis.title=element_blank(), axis.ticks=element_blank(), axis.text=element_blank(), legend.position="none")
gp1<-ggplotGrob(p1)
gp2<-ggplotGrob(p2)
maxWidth <- grid::unit.pmax(gp1$widths[2:5], gp2$widths[2:5])
gp1$widths[2:5] <- as.list(maxWidth)
gp2$widths[2:5] <- as.list(maxWidth)
pdf("figs/wunifrac.pdf")
grid.arrange(gp1, gp2, ncol=1,heights=c(4/5,1/5,1/5))
dev.off()

#pca plot of rarefied data
pca.unifrac <- prcomp(wuni)
pdf("figs/wunifrac_screeplot.pdf")
screeplot(pca.unifrac)
dev.off()
pdf("figs/wunifrac_pca.pdf")
fviz_pca_ind(pca.unifrac, habillage=merge$Group, geom="point") + labs(title="Weighted Unifrac Distance")
dev.off()

#######
#DESEQ
#######
#after loading in your phyloseq object (here ps.dat), covert it to a deseq object
ps.deseq <- phyloseq_to_deseq2(philr.dat, ~ Group) #CHANGE ME to your desired grouping variable, can only handle 2 groups
desq <- DESeq(ps.deseq, test="Wald", fitType="parametric")
res <- results(desq, cooksCutoff=F)
sigtab <- res[which(res$padj < 0.01),] # set your desired p value cutoff (if you have a lot of taxa remaining, increase)
sigtab <- cbind(as(sigtab, "data.frame"), as(tax_table(ps.dat)[rownames(sigtab),], "matrix"))

x <- tapply(sigtab$log2FoldChange, sigtab$V3, function(x) max(x)) #CHANGE ME to the taxonomic level you want to color your graph by, here V3==Phylum
x <- sort(x, TRUE)
sigtab$V3 <- factor(as.character(sigtab$V3), levels=names(x)) #CHANGE ME to the taxonomic level you want to color your graph by, here V3==Phylum

x <- tapply(sigtab$log2FoldChange, sigtab$V7, function(x) max(x)) #CHANGE ME to the taxonomic level you want to collapse your ASVs by, here V6==Genus
x <- sort(x, TRUE)
sigtab$V7 <- factor(as.character(sigtab$V7), levels=names(x)) #CHANGE ME to the taxonomic level you want to collapse your ASVs by, here V6==Genus
pdf("figs/DESeq.pdf")
ggplot(sigtab, aes(x=V7, y=log2FoldChange, color=V3)) + geom_point(size=2) + theme(axis.text.x = element_text(angle = -90, hjust = 0, vjust=0.5)) + coord_flip() #CHANGE ME to the two taxonomic levels you chose
dev.off()

#################
#ALPHA DIVERSITY
################
#plot_richness(ps.dat, measures=c("Observed", "Shannon")) #alpha by
pdf("figs/adiv_group.pdf")
plot_richness(ps.dat, x="Group", measures=c("Observed", "Shannon")) + theme_minimal() + geom_boxplot(alpha=0.0)
dev.off()
adiv <- estimate_richness(ps.dat)
# get p value for Control v PKU
wilcox.test(adiv[grepl("C", rownames(adiv)),]$Observed, adiv[grepl("P", rownames(adiv)),]$Observed)
##data:  adiv[grepl("C", rownames(adiv)), ]$Observed and adiv[grepl("P", rownames(adiv)), ]$Observed
##W = 143, p-value = 0.284
##alternative hypothesis: true location shift is not equal to 0

wilcox.test(adiv[grepl("C", rownames(adiv)),]$Shannon, adiv[grepl("P", rownames(adiv)),]$Shannon)
##data:  adiv[grepl("C", rownames(adiv)), ]$Shannon and adiv[grepl("P", rownames(adiv)), ]$Shannon
##W = 139, p-value = 0.3673
##alternative hypothesis: true location shift is not equal to 0

pdf("figs/adiv_sex.pdf")
plot_richness(ps.dat, x="Sex", measures=c("Observed", "Shannon")) + theme_minimal() + geom_boxplot(alpha=0.0)
dev.off()
adiv <- estimate_richness(ps.dat)
##          Observed Chao1 se.chao1 ACE   se.ACE  Shannon   Simpson InvSimpson
##Control03      132   132        0 132 3.040136 3.538829 0.9423110  17.334327
##Control04      242   242        0 242 5.126499 4.221533 0.9661580  29.549075
##Control07      167   167        0 167 3.066137 3.728391 0.9505190  20.209784
##Control08      119   119        0 119 3.026494 3.318331 0.9257340  13.465118
##Control09      184   184        0 184 2.766256 3.674243 0.9327441  14.868582
##Control10      227   227        0 227 4.172492 4.125465 0.9691035  32.366152
##Control11      237   237        0 237 3.094844 4.347347 0.9745783  39.336457
##Control12      188   188        0 188 4.319057 3.870059 0.9548606  22.153604
##Control13      223   223        0 223 3.962832 4.107648 0.9728138  36.783345
##Control14      205   205        0 205 3.728597 4.107196 0.9675680  30.833767
##Control16      180   180        0 180 4.024922 3.617866 0.9375750  16.019217
##Control19      240   240        0 240 3.506542 3.941268 0.9444432  17.999601
##Control20      218   218        0 218 3.737364 3.966544 0.9608759  25.559721
##Control22      145   145        0 145 3.188341 3.180344 0.9009509  10.096002
##Control23      204   204        0 204 3.360672 3.023701 0.8581324   7.048825
##Control25      185   185        0 185 3.216511 3.870190 0.9584639  24.075461
##Control26      223   223        0 223 3.622303 4.097844 0.9621230  26.401269
##Control28      152   152        0 152 2.752989 3.942852 0.9660269  29.435079
##Control29      172   172        0 172 3.466618 3.450147 0.9200602  12.509411
##Control30      188   188        0 188 3.351722 3.799578 0.9291157  14.107505
##Control4B      169   169        0 169 3.910235 3.596144 0.9346323  15.298081
##PKU02          203   203        0 203 2.599735 4.136786 0.9711155  34.620655
##PKU03          113   113        0 113 3.151064 3.340548 0.9387258  16.320069
##PKU10          213   213        0 213 5.005162 4.280387 0.9756902  41.135607
##PKU11          107   107        0 107 2.379822 3.703361 0.9595627  24.729637
##PKU14          158   158        0 158 3.454038 3.507885 0.9231935  13.019729
##PKU15A         198   198        0 198 3.834980 3.965117 0.9588355  24.292783
##PKU17          105   105        0 105 2.718543 2.839769 0.8775292   8.165212
##PKU21          248   248        0 248 4.741291 3.359627 0.8986739   9.869125
##PKU26          228   228        0 228 4.547508 3.818443 0.9511365  20.465161
##PKU30          176   176        0 176 4.116900 3.637947 0.9374928  15.998165
##PKU31          118   118        0 118 3.401146 3.494287 0.9446637  18.071305
##            Fisher
##Control03 15.98693
##Control04 32.16330
##Control07 18.86886
##Control08 13.09719
##Control09 19.86051
##Control10 25.70396
##Control11 27.36196
##Control12 21.57917
##Control13 23.95214
##Control14 23.30348
##Control16 20.84906
##Control19 28.49903
##Control20 25.53781
##Control22 16.09333
##Control23 21.87418
##Control25 20.28898
##Control26 26.64865
##Control28 18.51924
##Control29 20.04277
##Control30 23.00766
##Control4B 21.19850
##PKU02     23.68370
##PKU03     12.94790
##PKU10     27.30401
##PKU11     13.79196
##PKU14     19.42327
##PKU15A    23.28461
##PKU17     11.81639
##PKU21     27.58914
##PKU26     25.94815
##PKU30     21.83016
##PKU31     14.60676

# get p value for Control v PKU
wilcox.test(adiv[grepl("C", rownames(adiv)),]$Observed, adiv[grepl("P", rownames(adiv)),]$Observed)
##data:  adiv[grepl("C", rownames(adiv)), ]$Observed and adiv[grepl("P", rownames(adiv)), ]$Observed
##W = 143, p-value = 0.284
##alternative hypothesis: true location shift is not equal to 0
wilcox.test(adiv[grepl("C", rownames(adiv)),]$Shannon, adiv[grepl("P", rownames(adiv)),]$Shannon)
##data:  adiv[grepl("C", rownames(adiv)), ]$Shannon and adiv[grepl("P", rownames(adiv)), ]$Shannon
##W = 139, p-value = 0.3673
##alternative hypothesis: true location shift is not equal to 0

pdf("figs/adiv_observed_group.pdf")
plot_richness(ps.dat, x="Group", measures=c("Observed")) + theme_minimal() + geom_boxplot(alpha=0.0) + ylim(0,350)
dev.off()

pdf("figs/adiv_shannon_group.pdf")
plot_richness(ps.dat, x="Group", measures=c("Shannon")) + theme_minimal() + geom_boxplot(alpha=0.0) + ylim(1,6)
dev.off()

###########
#LIBRARIES
###########
library(tidyverse)
library(ggplot2)
library(ggdendro)
library(ape)
library(RColorBrewer)
library(vegan)
library(phylofactor)
library(ggtree)

##########
#ENV SETUP
##########
PATH="/home/lymelab/Desktop/PKU2018_adults/"# CHANGE ME to your working directory
setwd(PATH)
cols <- brewer.pal(6, "Set2")

###########
#LOAD DATA
###########
#load sample data
rawmetadata <- read_delim(file = file.path(PATH, "map.txt"), # file.path() is used for cross-platform compatibility
                          "\t", # the text file is tab delimited
                          escape_double = FALSE, # the imported text file does not 'escape' quotation marks by wrapping them with more quotation marks
                          trim_ws = TRUE) # remove leading and trailing spaces from character string entries
seqtab.filtered <- read.table("sequence_table.16s.filtered.txt", header=T, row.names=1)
system("sed 's/\t/;/g' taxonomy_L7.txt | sed 's/;/\t/' > taxonomy_phylofactor.txt")
taxa <- read.table("taxonomy_phylofactor.txt", header=F, sep="\t", row.names=1)
notinmeta <- setdiff(row.names(seqtab.filtered), rawmetadata$SampleID) #record samples absent in either metadata or OTU table
notinraw <- setdiff(rawmetadata$SampleID, row.names(seqtab.filtered))
tree <- read.tree("rep_set.filt.root.tre") #load representative tree

##############
#Phylofactor
##############
OTUTable <- as.matrix(t(seqtab.filtered)) #convert your sequence table into a matrix

#the next few commands just make sure that the sample names in your OTU table match those in your mapping file
filt.list <- colnames(OTUTable)
filtmap <- rawmetadata[rawmetadata$SampleID %in% filt.list,]
filtmap <- filtmap[match(filt.list, filtmap$SampleID),]
x <- as.factor(filtmap$Group) # CHANGE ME to your variable that you want to group by
tax <- read.table("taxonomy_phylofactor.txt", sep="\t", header=F)
common.otus <- which(rowSums(OTUTable>0)>10) # CHANGE ME the current code filters out any ASV that isn't found in at least 10 samples
OTUTable <- OTUTable[common.otus,] #filter out low abundance ASVs from table
tree <- ape::drop.tip(tree, setdiff(tree$tip.label, rownames(OTUTable))) #remove low abundance ASVs from tree
PF <- PhyloFactor(OTUTable, tree, x, nfactors=3 , choice='F') # CHANGE ME to the number of "factors" you want the program to find (here 3)
PF$Data <- PF$Data[PF$tree$tip.label,] #this fixes the warning message that comes out of previous command
gtree <- pf.tree(PF,layout="rectangular") #generate tree (leave out full layout parameter for a circular tree)
pdf("figs/phylofactor_tree_0820.pdf")
gtree$ggplot + geom_tiplab() #plot
dev.off()
gtree <- pf.tree(PF, layout="rectangular") #generate tree (leave out full layout parameter for a circular tree)
pdf("figs/phylofactor_notip_tree_0820.pdf")
gtree$ggplot #plot
dev.off()
gtree <- pf.tree(PF) #generate tree (leave out full layout parameter for a circular tree)
pdf("figs/phylofactor_circle_tree_0820.pdf")
gtree$ggplot + geom_tiplab() #plot
dev.off()
gtree <- pf.tree(PF) #generate tree (leave out full layout parameter for a circular tree)
pdf("figs/phylofactor_circnotip_tree_0820.pdf")
gtree$ggplot #plot
dev.off()

#Generate boxplots, calculate p value for each comparison
#factor one
y <- t(PF$basis[,1]) %*% log(PF$Data)
dat <- as.data.frame(cbind(as.matrix(PF$X), (t(y))))
dat$V2 <- as.numeric(as.character(dat$V2))
pdf("figs/factor1_boxp_0920.pdf")
ggplot(dat, aes(x=dat$V1, y=dat$V2)) + geom_boxplot(fill=gtree$legend$colors[1]) + theme_classic() + ylab("ILR abundance") + xlab("") + ggtitle('Factor 1') + ylim(c(-10,10))
dev.off()

#wilcox test for two category comparisons, use something like anova for multiple groups
wilcox.test(dat[dat$V1 == "PKU",]$V2, dat[dat$V1 == "Control",]$V2)
##data:  dat[dat$V1 == "PKU", ]$V2 and dat[dat$V1 == "Control", ]$V2
##W = 19, p-value = 3.049e-05
##alternative hypothesis: true location shift is not equal to 0

#factor two
y <- t(PF$basis[,2]) %*% log(PF$Data)
dat <- as.data.frame(cbind(as.matrix(PF$X), (t(y))))
dat$V2 <- as.numeric(as.character(dat$V2))
pdf("figs/factor2_boxp_0920.pdf")
ggplot(dat, aes(x=dat$V1, y=dat$V2)) + geom_boxplot(fill=gtree$legend$colors[2]) + theme_classic() + ylab("ILR abundance") + xlab("") + ggtitle('Factor2') + ylim(c(-10,10))
dev.off()

#wilcox test for two category comparisons, use something like anova for multiple groups
wilcox.test(dat[dat$V1 == "PKU",]$V2, dat[dat$V1 == "Control",]$V2)
##data:  dat[dat$V1 == "PKU", ]$V2 and dat[dat$V1 == "Control", ]$V2
##W = 47, p-value = 0.005567
##alternative hypothesis: true location shift is not equal to 0

#factor three
y <- t(PF$basis[,3]) %*% log(PF$Data)
dat <- as.data.frame(cbind(as.matrix(PF$X), (t(y))))
dat$V2 <- as.numeric(as.character(dat$V2))
pdf("figs/factor3_boxp_0920.pdf")
ggplot(dat, aes(x=dat$V1, y=dat$V2)) + geom_boxplot(fill=gtree$legend$colors[3]) + theme_classic() + ylab("ILR abundance") + xlab("") + ggtitle('Factor3') + ylim(c(-10,10))
dev.off()

#wilcox test for two category comparisons, use something like anova for multiple groups
wilcox.test(dat[dat$V1 == "PKU",]$V2, dat[dat$V1 == "Control",]$V2)
##data:  dat[dat$V1 == "PKU", ]$V2 and dat[dat$V1 == "Control", ]$V2
##W = 192, p-value = 0.001691
##alternative hypothesis: true location shift is not equal to 0


#some information about the factors that were pulled from your data
PF$factors
##                              Group1                        Group2        F
##Factor 1                         tip 200 member Monophyletic clade 24.73057
##Factor 2 2 member Monophyletic clade 198 member Paraphyletic clade 14.64195
##Factor 3                         tip 197 member Paraphyletic clade 13.39050
##               Pr(>F)
##Factor 1 2.514315e-05
##Factor 2 6.137492e-04
##Factor 3 9.647645e-04



########################
#RAREFACTION CURVE PLOT
########################
pdf("figs/rarefaction_curve.pdf")
rarecurve(seqtab.filtered, label=FALSE)
dev.off()


#########
#picrust
#########
#in terminal, navigate to wd
conda activate picrust2
awk 'NF{NF-=2}1' FS='\t' OFS='\t' sequence_taxonomy_table.16s.merged.txt > sequence_table.forpicrust.txt
picrust2_pipeline.py -s rep_set_fix.fa -i sequence_table.forpicrust.txt -o picrust -p 6
#vizualization via STAMP
